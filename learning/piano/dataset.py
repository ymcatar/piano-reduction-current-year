'''
Handles the caching of note features generated by the Reducer.

The cache must be cleared manually when the algorithms themselves are changed.
'''

from collections import defaultdict
from collections.abc import Sequence
import hashlib
import h5py
import json
import logging
import numpy as np
import os
import os.path
from .score import ScoreObject
from .util import dump_algorithm
from .contraction import ContractionMapping


CACHE_DIR = 'sample/cache'


DEFAULT_SAMPLES = [
    'sample/input/i_0000_Beethoven_op18_no1-4.xml:sample/output/o_0000_Beethoven_op18_no1-4.xml',
    'sample/input/i_0001_Spring_sonata_I.xml:sample/output/o_0001_Spring_sonata_I.xml',
    'sample/input/i_0002_Beethoven_Symphony_No5_Mov1.xml:sample/output/o_0002_Beethoven_Symphony_No5_Mov1.xml',
    'sample/input/i_0003_Beethoven_Symphony_No7_Mov2.xml:sample/output/o_0003_Beethoven_Symphony_No7_Mov2.xml',
    'sample/input/i_0004_Mozart_Symphony_No25.xml:sample/output/o_0004_Mozart_Symphony_No25.xml',
    'sample/input/i_0005_Mozart_Symphony_No40.xml:sample/output/o_0005_Mozart_Symphony_No40.xml',
    'sample/input/i_0006_Dvorak_New_World_Symphony_No9_Mov2.xml:sample/output/o_0006_Dvorak_New_World_Symphony_No9_Mov2.xml',
    'sample/input/i_0007_Tchaikovsky_nutcracker_march.xml:sample/output/o_0007_Tchaikovsky_nutcracker_march.xml'
    ]


CROSSVAL_SAMPLES = [
    'sample/input/i_0002_Beethoven_Symphony_No5_Mov1.xml:sample/output/o_0002_Beethoven_Symphony_No5_Mov1.xml',
    'sample/input/i_0003_Beethoven_Symphony_No7_Mov2.xml:sample/output/o_0003_Beethoven_Symphony_No7_Mov2.xml',
    'sample/input/i_0004_Mozart_Symphony_No25.xml:sample/output/o_0004_Mozart_Symphony_No25.xml',
    'sample/input/i_0005_Mozart_Symphony_No40.xml:sample/output/o_0005_Mozart_Symphony_No40.xml',
    'sample/input/i_0006_Dvorak_New_World_Symphony_No9_Mov2.xml:sample/output/o_0006_Dvorak_New_World_Symphony_No9_Mov2.xml',
    'sample/input/i_0007_Tchaikovsky_nutcracker_march.xml:sample/output/o_0007_Tchaikovsky_nutcracker_march.xml'
    ]


def hash_file(path):
    hasher = hashlib.sha256()
    with open(path, 'rb') as f:
        for block in iter(lambda: f.read(65536), b''):
            hasher.update(block)
    return hasher.hexdigest()


def describe_algorithm(algo):
    args = ['algorithm', *dump_algorithm(algo)]
    return 'cache:' + json.dumps(args, sort_keys=True)


def set_in_cache(cache, description, value):
    # Since dataset names have limited length, we use the attrs dict to index
    # the dataset name for each description
    if description in cache.attrs:
        key = cache.attrs[description]
        del cache[key]
        cache[key] = value
    else:
        serial = 0
        while any(i == '{}'.format(serial) for i in cache.attrs.values()):
            serial += 1
        key = '{}'.format(serial)
        cache.attrs[description] = key
        cache[key] = value


class DatasetEntry:
    '''
    Class that handles loading of an input score or an input-output score pair.
    To load an input score only, set the output part of the path_pair or
    score_obj_pair to be None.
    '''
    def __init__(self, path_pair=None, score_obj_pair=None):
        self.X = None
        self.y = None
        self.contractions = {}
        self.structures = {}
        assert (path_pair is None) != (score_obj_pair is None), \
            'exactly one of path_pair and score_obj_pair must be provided!'

        if score_obj_pair:
            self.input_score_obj, self.output_score_obj = score_obj_pair
            self.in_path = self.out_path = None
        else:
            self.in_path, self.out_path = path_pair
            self.input_score_obj = self.output_score_obj = None

    @property
    def loaded(self):
        return self.X is not None

    @property
    def has_output(self):
        return bool(self.output_score_obj or self.out_path)

    @property
    def _name(self):
        if self.in_path:
            return os.path.basename(self.in_path)
        else:
            return '<{}>'.format(self.input_score_obj.score.metadata.title or 'Score')

    def _load_marking(self, reducer, algo, *, use_cache, cache):
        description = describe_algorithm(algo)
        if cache and use_cache and description in cache.attrs:
            ds = cache[cache.attrs[description]]
            if self.input_score_obj:
                for i, key in enumerate(algo.all_keys):
                    self.input_score_obj.annotate(ds[:, i], key)
        else:
            self.ensure_scores_loaded()
            logging.info('Evaluating {}'.format(type(algo).__name__))
            algo.run(self.input_score_obj)
            ds = np.hstack(
                self.input_score_obj.extract(key, dtype='float', default=0)[:, np.newaxis]
                for key in algo.all_keys)
            if cache:
                set_in_cache(cache, description, ds)

        for i, key in enumerate(algo.all_keys):
            self.X[:, reducer.all_keys.index(key)] = ds[:, i]

    def _load_contraction(self, reducer, algo, *, use_cache, cache):
        description = describe_algorithm(algo)
        if cache and use_cache and description in cache.attrs:
            ds = cache[cache.attrs[description]]
        else:
            self.ensure_scores_loaded()
            logging.info('Evaluating contraction {}'.format(type(algo).__name__))
            ds = list(algo.run(self.input_score_obj))
            if cache:
                set_in_cache(cache, description, ds)
        self.contractions[algo.key] = [((r[0], r[1]), ()) for r in ds]

        return ds

    def _load_structure(self, reducer, algo, *, use_cache, cache):
        description = describe_algorithm(algo)
        if cache and use_cache and description in cache.attrs:
            ds = cache[cache.attrs[description]]
        else:
            self.ensure_scores_loaded()
            logging.info('Evaluating structure {}'.format(type(algo).__name__))
            # Concatenate to simplify caching
            ds = np.stack([np.concatenate(x, axis=0)
                           for x in algo.run(self.input_score_obj)])
            if cache:
                set_in_cache(cache, description, ds)
        self.structures[algo.key] = [((int(r[0]), int(r[1])), r[2:]) for r in ds]

        return ds

    def _load_alignment_marking(self, reducer, algo, *, extra=False, use_cache, cache):
        description = describe_algorithm(algo)
        if cache and use_cache and description in cache.attrs:
            ds = cache[cache.attrs[description]]
            if self.input_score_obj:
                self.input_score_obj.annotate(ds, algo.key)
        else:
            self.ensure_scores_loaded()
            logging.info('Evaluating alignment')
            algo.run(self.input_score_obj, self.output_score_obj, extra=extra)
            ds = self.input_score_obj.extract(algo.key, dtype='int')
            if cache:
                set_in_cache(cache, description, ds)

        self.y[:, 0] = ds

    def ensure_scores_loaded(self):
        if self.input_score_obj:
            return
        logging.info('Loading score')
        self.input_score_obj = ScoreObject.from_file(self.in_path)
        if self.has_output:
            self.output_score_obj = ScoreObject.from_file(self.out_path)

    def load(self, reducer, extra=False, use_cache=False, keep_scores=False):
        logging.info('Loading {}'.format(self._name))

        # Do not remove already loaded scores
        keep_scores = keep_scores or (self.input_score_obj is not None)

        # Use cache requires loading from file
        use_cache = use_cache and self.in_path is not None

        # Write cache whenever loading from file
        write_cache = self.in_path is not None

        if keep_scores:
            self.ensure_scores_loaded()

        cache = None
        try:
            if use_cache or write_cache:
                cache_attrs = {
                    'input_sha256': hash_file(self.in_path),
                    }
                if self.has_output:
                    cache_attrs['output_sha256'] = hash_file(self.out_path)
                cache_path = (
                    CACHE_DIR + '/' + os.path.basename(self.in_path).rsplit('.', 1)[0] + '.hdf5')
                if not os.path.exists(CACHE_DIR):
                    os.mkdir(CACHE_DIR)

                cache = h5py.File(cache_path, 'a')
                if (not all(cache.attrs.get(k) == v for k, v in cache_attrs.items()) or
                        'len' not in cache.attrs):
                    logging.info('Invalidating all cache')
                    cache.close()
                    cache = None
                    cache = h5py.File(cache_path, 'w')
                    cache.attrs.update(cache_attrs)

            if cache and 'len' in cache.attrs:
                n = cache.attrs['len']
            else:
                self.ensure_scores_loaded()
                n = len(self.input_score_obj)
                if cache:
                    cache.attrs['len'] = n

            load_options = {
                'use_cache': use_cache,
                'cache': cache,
                }

            contractions = [
                c for algo in reducer.contractions
                for c in self._load_contraction(reducer, algo, **load_options)]
            self.mapping = ContractionMapping(contractions, n)
            self.len = self.mapping.output_size
            if self.len != n:
                logging.info('Contractions: {} notes => {} notes'.format(n, self.len))

            self.X = np.empty((n, len(reducer.all_keys)), dtype='float')
            for algo in reducer.algorithms:
                self._load_marking(reducer, algo, **load_options)
            self.X = self.mapping.map_matrix(self.X)

            n_edge_features = sum(a.n_features for a in reducer.structures)
            features = {}
            d = 0
            structure = defaultdict(lambda: np.zeros(n_edge_features, dtype='float'))
            for algo in reducer.structures:
                data = self._load_structure(reducer, algo, **load_options)
                for row in data:
                    edge, features = row[:2].astype('int'), row[2:]
                    structure[tuple(sorted(edge))][d:d + algo.n_features] = features
                d += algo.n_features
            structure = self.mapping.map_structure(structure)
            self.E = np.empty((len(structure), 2), dtype='int')
            self.F = np.empty((len(structure), n_edge_features), dtype='float')
            for i, (k, v) in enumerate(structure.items()):
                self.E[i] = k
                self.F[i] = v

            if self.has_output:
                self.y = np.empty((n, 1), dtype='int')
                self._load_alignment_marking(reducer, reducer.alignment, extra=extra,
                                             **load_options)
                self.y = self.mapping.map_matrix(self.y)
        finally:
            if cache:
                cache.close()

        if not keep_scores:
            # Allow garbage collection
            self.input_score_obj = self.output_score_obj = None


class Dataset:
    def __init__(self, reducer, paths=None, entries=None, use_cache=False, keep_scores=False):
        if entries:
            assert not paths
            self.entries = entries
        else:
            if paths is None:
                paths = DEFAULT_SAMPLES
            self.entries = [DatasetEntry(p.split(':', 1)) for p in paths]

        self.reducer = reducer
        self.options = {
            'use_cache': use_cache,
            'keep_scores': keep_scores,
            }
        self.use_cache = use_cache
        self.keep_scores = keep_scores

    def __len__(self):
        return len(self.entries)

    def __getitem__(self, index):
        entries = self.entries[index]

        return Dataset(self.reducer, entries=entries, **self.options)

    def get_matrices(self, structured=False):
        Xys = [self.load(i, structured=structured) for i in range(len(self))]
        Xs = [X for X, _ in Xys]
        ys = [y for _, y in Xys]

        if not structured:
            X = np.vstack(Xs)
            y = np.vstack(ys)
            return X, y
        else:
            return Xs, ys

    def split_dataset(self, index):
        '''
        Split the dataset into training and validation sets, where scores at
        `index` are put into the validation set.
        '''
        if isinstance(index, Sequence):
            indices = set(index)
        else:
            indices = {index}

        for i in range(len(self)):
            self.load(i)

        train_entries = [e for i, e in enumerate(self.entries) if i not in indices]
        valid_entries = [e for i, e in enumerate(self.entries) if i in indices]

        return (Dataset(self.reducer, entries=train_entries, **self.options),
                Dataset(self.reducer, entries=valid_entries, **self.options))

    def load(self, index, structured=False):
        entry = self.entries[index]
        if not entry.loaded:
            entry.load(self.reducer, **self.options)

        if not structured:
            return entry.X, entry.y
        else:
            return (entry.X, entry.E, entry.F), entry.y

    def find_index(self, paths):
        for i, entry in enumerate(self.entries):
            if '{}:{}'.format(entry.in_path, entry.out_path) == paths:
                return i
        raise IndexError('Sample "{}" does not exist'.format(paths))


def clear_cache(substring=None):
    '''
    Clear all cache entries whose algorithm class matches the provided
    substring. If a substring is not provided, all cache is purged.
    '''
    if substring is None:
        logging.info('Purging all cache')
        removed = 0
        for f in os.listdir(CACHE_DIR):
            if f.endswith('.hdf5'):
                os.remove(os.path.join(CACHE_DIR, f))
                removed += 1
        logging.info('Removed {} cache files'.format(removed))
    else:
        logging.info('Purging cache matching "{}"'.format(substring))
        touched_count = 0
        removed = 0
        for fname in os.listdir(CACHE_DIR):
            if fname.endswith('.hdf5'):
                with h5py.File(os.path.join(CACHE_DIR, fname), 'a') as f:
                    touched = False
                    for k in list(f.attrs):
                        if k.startswith('cache:'):
                            description = json.loads(k[len('cache:'):])
                            if substring in description[1]:
                                if f.attrs[k] in f:
                                    del f[f.attrs[k]]
                                del f.attrs[k]
                                removed += 1
                                touched = True

                    touched_count += touched
        logging.info('Removed {} cache entries from {} files'.format(removed, touched_count))
